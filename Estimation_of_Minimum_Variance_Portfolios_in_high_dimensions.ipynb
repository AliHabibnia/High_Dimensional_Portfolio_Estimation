{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Simiulation\n",
        "\n",
        "Comparison of various MVP estimators including linear and nonlinear shrinkage methods. The mean and standard deviation (in parentheses) based on 1,000 replications of the Relative Risk ratio of estimated MVPs are reported. The number of assets is N = 84 and the sampling sizes are T = 50, 100\n",
        "and 200. Monte Carlo with the assumption of normality."
      ],
      "metadata": {
        "id": "WorJfyeq6IIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bs4 > NUL\n",
        "!pip install yfinance > NUL\n",
        "!pip install non-linear-shrinkage > NUL\n",
        "!pip install tabulate > NUL"
      ],
      "metadata": {
        "id": "A6nvpdPy8dd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "import bs4 as bs\n",
        "import datetime\n",
        "import random\n",
        "import yfinance as yf\n",
        "import pandas_datareader as web\n",
        "import matplotlib.pyplot as plt\n",
        "from tabulate import tabulate\n",
        "import seaborn as sns\n",
        "from scipy.integrate import quad\n",
        "import statsmodels.formula.api as sm\n",
        "from scipy.stats import multivariate_normal\n",
        "from scipy.linalg import inv\n",
        "import nonlinshrink as nls\n",
        "from sklearn.covariance import GraphicalLasso, GraphicalLassoCV, LedoitWolf, ShrunkCovariance, OAS"
      ],
      "metadata": {
        "id": "ZccfpirI-1-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_sizes = [50,100,200]\n",
        "methods = ['PlugIn', 'linear_shrinkage', 'non_linear_shrinkage', 'equally_weighted']\n",
        "\n",
        "def Risk_Ratio_RR(covRe_df, estimated_portfolio_weights):\n",
        "    '''\n",
        "    evaluation is based on risk ratio (R.R.) defined as the ratio between the standard\n",
        "    deviation of the return of an estimated portfolio, w^, and the minimum standard\n",
        "    deviation.\n",
        "    '''\n",
        "    given_covariance_matrix = covRe_df\n",
        "    inverse_given_covariance_matrix = inv(covRe_df)\n",
        "    identity_matrix = np.ones(len(covRe_df))\n",
        "\n",
        "    # Nominator Risk R(W^)\n",
        "    nominator_risk = estimated_portfolio_weights.T @ given_covariance_matrix @ estimated_portfolio_weights\n",
        "\n",
        "    # Denominator Risk R(W*) -> Rmin\n",
        "    denominator_risk = 1 /(identity_matrix @ inverse_given_covariance_matrix @ identity_matrix)\n",
        "\n",
        "    return np.sqrt((nominator_risk / denominator_risk))\n",
        "\n",
        "\n",
        "def prepare_multivariate_parameters(start_date=datetime.datetime(2004, 1, 1), end_date=datetime.datetime(2016, 1, 1),\n",
        "                                     number_of_companies=None, seed=None, covariance_method='graphicallassocv',\n",
        "                                     pre_downloaded_data=None):\n",
        "    # Set seed for reproducibility\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "        random.seed(seed)\n",
        "\n",
        "    # Getting the S&P 100 tickers from Wikipedia\n",
        "    resp = requests.get('https://en.wikipedia.org/wiki/S%26P_100')\n",
        "    soup = bs.BeautifulSoup(resp.text, 'lxml')\n",
        "    table = soup.find('table', {'class': 'wikitable sortable'})\n",
        "    tickers = [row.findAll('td')[0].text.replace('\\n', '') for row in table.findAll('tr')[1:]]\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "    if number_of_companies is not None:\n",
        "        tickers = np.random.choice(tickers, number_of_companies, replace=False)\n",
        "\n",
        "    # Downloading data from Yahoo finance\n",
        "    if pre_downloaded_data is None:\n",
        "        data = yf.download(tickers, start=start_date, end=end_date)['Adj Close']\n",
        "        cleaned_data = data.dropna(axis='columns', how='any').dropna(axis='index', how='any')\n",
        "    else:\n",
        "        cleaned_data = pre_downloaded_data\n",
        "\n",
        "    Re = (np.log(cleaned_data.iloc[:, :-1]) - np.log(cleaned_data.iloc[:, :-1].shift(1))).dropna()\n",
        "    # Reading factor data from Fama-French\n",
        "    df_factors = web.DataReader('F-F_Research_Data_5_Factors_2x3_daily', 'famafrench', start=start_date, end=end_date)[0]\n",
        "    df_factors.rename(columns={'Mkt-RF': 'MKT'}, inplace=True)\n",
        "    df_factors['MKT'] = df_factors['MKT'] / 100\n",
        "    ##  Market return for S&P100 can also be retrived directly from Yahoo\n",
        "    #market_returns = yf.download('^OEX', start=start_date, end=end_date)['Adj Close'].pct_change().dropna()\n",
        "\n",
        "    # Compute CAPM and residuals\n",
        "    coeffs = np.empty((Re.shape[1],1))\n",
        "    residuals = np.empty((len(Re), 0))\n",
        "    for i in range(Re.shape[1]):\n",
        "        df_stk = Re.iloc[:, i]\n",
        "        df_stock_factor = pd.merge(df_stk, df_factors, left_index=True, right_index=True)\n",
        "        df_stock_factor['XsRet'] = df_stock_factor.iloc[:, 0] - df_stock_factor['RF']\n",
        "        CAPM = sm.ols(formula='XsRet ~ MKT', data=df_stock_factor).fit(cov_type='HAC', cov_kwds={'maxlags': 1})\n",
        "        coeffs[i] = CAPM.params[1]\n",
        "        residuals = np.hstack((residuals, CAPM.resid.values.reshape(-1, 1)))\n",
        "\n",
        "    # Estimation of covU, sigma2_m, and covRe\n",
        "\n",
        "    # Compute covU based on the selected method\n",
        "    if covariance_method == 'lw':\n",
        "        lw = LedoitWolf().fit(residuals)\n",
        "        covU = lw.covariance_\n",
        "    elif covariance_method == 'sc':\n",
        "        sc = ShrunkCovariance().fit(residuals)\n",
        "        covU = sc.covariance_\n",
        "    elif covariance_method == 'oas':\n",
        "        oas = OAS().fit(residuals)\n",
        "        covU = oas.covariance_\n",
        "    elif covariance_method == 'pds':\n",
        "        sc = ShrunkCovariance().fit(residuals)\n",
        "        oas = OAS().fit(residuals)\n",
        "        alpha = 0.5  # regularization parameter\n",
        "        covU = alpha * np.abs(sc.covariance_) + (1 - alpha) * oas.covariance_\n",
        "    elif covariance_method == 'graphicallassocv':\n",
        "        model_cov = GraphicalLassoCV()\n",
        "        covU = model_cov.fit(residuals).covariance_\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown covariance_method: {covariance_method}\")\n",
        "    sigma2_m = np.var(df_factors['MKT'])\n",
        "    B = coeffs.reshape(-1, 1)\n",
        "    covRe = sigma2_m * B @ B.T + covU  # Estimated covariance matrix for returns\n",
        "\n",
        "    return covRe, len(covRe)\n",
        "\n",
        "def generate_multivariate_returns2(covRe, sample_size):\n",
        "    # Generate multivariate returns\n",
        "    rv = multivariate_normal(mean=np.zeros(len(covRe)), cov=covRe)\n",
        "    multivariate_returns = rv.rvs(size=sample_size)\n",
        "    # Convert to DataFrame\n",
        "    return_series = pd.DataFrame(multivariate_returns, columns=[f'Asset_{i}' for i in range(1, len(covRe) + 1)])\n",
        "\n",
        "    return return_series\n",
        "\n",
        "\n",
        "covRe, length_covRe = prepare_multivariate_parameters(covariance_method='pds')\n",
        "\n",
        "# Create a DataFrame to hold the results\n",
        "results_df = pd.DataFrame(columns=['Portfolio', 'T=50', 'T=100', 'T=200'])\n",
        "\n",
        "# Loop through each method and sample size\n",
        "for method in methods:\n",
        "    row_data = [method]\n",
        "    for sample_size in sample_sizes:\n",
        "        risk_ratios = [] # List to store the risk ratios\n",
        "\n",
        "        for _ in range(1000): # Run 1000 simulations\n",
        "            return_series = generate_multivariate_returns2(covRe, sample_size=sample_size)\n",
        "            identity_matrix = np.ones(len(covRe))\n",
        "            sample_cov_matrix = return_series.cov()\n",
        "\n",
        "            # Calculate the estimated portfolio weights based on the selected method\n",
        "            if method == 'PlugIn':\n",
        "                estimated_portfolio_weights = (inv(sample_cov_matrix) @ identity_matrix) / (identity_matrix.T @ inv(sample_cov_matrix) @ identity_matrix)\n",
        "            elif method == 'linear_shrinkage':\n",
        "                shrunk_cov = LedoitWolf().fit(return_series).covariance_\n",
        "                estimated_portfolio_weights = (inv(shrunk_cov) @ identity_matrix) / (identity_matrix.T @ inv(shrunk_cov) @ identity_matrix)\n",
        "            elif method == 'non_linear_shrinkage':\n",
        "                sigma_tilde = nls.shrink_cov(return_series)\n",
        "                estimated_portfolio_weights = (inv(sigma_tilde) @ identity_matrix) / (identity_matrix.T @ inv(sigma_tilde) @ identity_matrix)\n",
        "            elif method == 'equally_weighted':\n",
        "                cov_matrix = np.eye(len(covRe), len(covRe))\n",
        "                estimated_portfolio_weights = (inv(cov_matrix) @ identity_matrix) / (identity_matrix.T @ inv(cov_matrix) @ identity_matrix)\n",
        "\n",
        "            risk_ratio = Risk_Ratio_RR(covRe, estimated_portfolio_weights)\n",
        "            risk_ratios.append(risk_ratio)\n",
        "\n",
        "        average_risk_ratio = np.mean(risk_ratios)\n",
        "        std_dev_risk_ratio = np.std(risk_ratios)\n",
        "\n",
        "        row_data.append(f\"{average_risk_ratio} ({std_dev_risk_ratio})\")\n",
        "\n",
        "    results_df.loc[len(results_df)] = row_data\n",
        "\n",
        "table = tabulate(results_df, headers='keys', tablefmt='pipe', stralign='center')\n",
        "print(table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnkDGC1DbAsf",
        "outputId": "f47d7775-dab7-4636-d401-dff164b0b26c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*********************100%%**********************]  101 of 101 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:yfinance:\n",
            "2 Failed downloads:\n",
            "ERROR:yfinance:['DOW']: Exception(\"%ticker%: Data doesn't exist for startDate = 1072933200, endDate = 1451624400\")\n",
            "ERROR:yfinance:['BRK.B']: Exception('%ticker%: No timezone found, symbol may be delisted')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "|    |      Portfolio       |                    T=50                    |                   T=100                    |                   T=200                    |\n",
            "|---:|:--------------------:|:------------------------------------------:|:------------------------------------------:|:------------------------------------------:|\n",
            "|  0 |        PlugIn        |   50.97504963128382 (344.02438671557593)   |  2.4417100315376876 (0.39395618169259256)  | 1.3043380719892483 (0.055065274129960116)  |\n",
            "|  1 |   linear_shrinkage   |  1.3166177336035694 (0.06674957834846579)  |  1.3320348248670821 (0.06111533112450627)  |  1.2088202774385117 (0.03319756207751076)  |\n",
            "|  2 | non_linear_shrinkage | 1.1992791153546836 (0.041514089820499256)  |  1.1641108700994716 (0.03253357126267914)  |  1.1148936146566744 (0.01986082849963617)  |\n",
            "|  3 |   equally_weighted   | 1.4938001886770529 (6.661338147750939e-16) | 1.4938001886770529 (6.661338147750939e-16) | 1.4938001886770529 (6.661338147750939e-16) |\n"
          ]
        }
      ]
    }
  ]
}